# ğŸš€ Data Pipeline E-Commerce

## ğŸ“… Contexte

Ce projet met en Å“uvre un pipeline de donnÃ©es complet pour une plateforme e-commerce fictive. Il assure l'orchestration, la validation, l'enrichissement, l'agrÃ©gation et la surveillance de qualitÃ© des donnÃ©es issues de plusieurs sources (sessions utilisateurs, produits, utilisateurs, logs API).

## ğŸ  Structure du Projet

```
.
â”œâ”€â”€ config/                  # SchÃ©mas, rÃ¨gles mÃ©tier et seuils de qualitÃ©
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Fichiers bruts
â”‚   â”œâ”€â”€ staging/            # Fichiers prÃ©-nettoyÃ©s en attente de validation
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ api_logs/       # DonnÃ©es apis_logs traitÃ©es
â”‚   â”‚   â”œâ”€â”€ enriched        # DonnÃ©es enrichies
â”‚   â”‚   â”œâ”€â”€ joined          # DonnÃ©es consolidÃ©es pour la BI par exemple
â”‚   â”‚   â”œâ”€â”€ products        # DonnÃ©es produits traitÃ©s
â”‚   â”‚   â”œâ”€â”€ sales           # DonnÃ©es des ventes traitÃ©es
â”‚   â”‚   â”œâ”€â”€ sessions        # DonnÃ©es des sessions traitÃ©es
â”‚   â”œâ”€â”€ quality/            # Rapports de qualitÃ© et alertes (json,html,csv,txt,etc..)
â”‚   â””â”€â”€ archive/            # DonnÃ©es archivÃ©es 
â”œâ”€â”€ orchestration/
â”‚   â”œâ”€â”€ pipeline_master.sh  # Orchestrateur principal
â”‚   â”œâ”€â”€ data_discovery.sh   # DÃ©tection des sources entrantes
â”‚   â”œâ”€â”€ worker_manager.sh   # Dispatch du traitement en parallÃ¨le
â”‚   â”œâ”€â”€ quality_monitor.sh  # Lancement des contrÃ´les de qualitÃ©
â”œâ”€â”€ processing/
â”‚   â”œâ”€â”€ data_validator.py   # Validation de qualitÃ© (schÃ©ma, rÃ¨gles, complÃ©tude)
â”‚   â”œâ”€â”€ api_log_processor.py  # Traitement des donnÃ©es utilisateur
â”‚   â”œâ”€â”€ busines_processor.py  # Traitement des donnÃ©es utilisateur
â”‚   â”œâ”€â”€ product_processor.py  # Traitement des donnÃ©es utilisateur
â”‚   â”œâ”€â”€ session_processor.py  # Traitement des donnÃ©es utilisateur
â”‚   â”œâ”€â”€ data_validator.py  # Traitement des donnÃ©es utilisateurâ”‚   â€¦
â”œâ”€â”€ transformations/
â”‚   â”œâ”€â”€ data_enricher.py    # Enrichissement (calculs, dÃ©rivÃ©es, KPI)
â”‚   â”œâ”€â”€ data_aggregator.py  # AgrÃ©gation multi-dimensionnelle
â”‚   â”œâ”€â”€ data_joiner.py      # Fusion des sources
â”‚   â”œâ”€â”€ data_formatter.py   # GÃ©nÃ©ration des rapports CSV/Excel
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ alert_manager.py    # Simulation des alertes qualitÃ©
â”‚   â”œâ”€â”€ dashboard_gen.py    # GÃ©nÃ©ration du dashboard HTML
â”œâ”€â”€ requirements.txt        # Librairies Python requises
â”œâ”€â”€ Makefile                # Environnement virtuel, outils CLI
â””â”€â”€ README.md               # Documentation
```

## ğŸšœ Workflow Global

1. **Initialisation** du pipeline (structure, variables, config)
2. **Scan** des nouvelles sources de donnÃ©es (raw -> staging)
3. **Traitement** des fichiers via workers Python
4. **ContrÃ´les QualitÃ©**
   - Validation de schÃ©ma (via `data_schemas.json`)
   - RÃ¨gles mÃ©tier (via `business_rules.yaml`)
   - DÃ©tection d'anomalies statistiques
   - ComplÃ©tude et seuil de qualitÃ© (`quality_thresholds.yaml`)
5. **Alertes** en cas d'Ã©chec dans `quality_alert.txt`
6. **Dashboard** HTML des rapports dans `data/processed/final/dashboard.html`
7. (Optionnel) **AgrÃ©gation et reporting** CSV/Excel
8. (Prochainement) **Archivage** et **Reprise** intelligente

## âš–ï¸ Orchestration Bash

Le script principal `pipeline_master.sh` gÃ¨re toutes les Ã©tapes :

```bash
./orchestration/pipeline_master.sh
```

## ğŸ§° QualitÃ© de DonnÃ©es

- Fichiers validÃ©s uniquement si tous les critÃ¨res sont respectÃ©s
- Rapports stockÃ©s au format JSON : `data/quality/validation_report_<file>.json`
- Fichiers rejetÃ©s listÃ©s dans : `quality_alert.txt`

## ğŸ“Š Dashboard QualitÃ©

Un fichier HTML synthÃ©tique est gÃ©nÃ©rÃ© dans `data/quality/dashboard.html` avec :

- Le statut de chaque fichier
- La complÃ©tude
- Les erreurs dÃ©tectÃ©es

## âš™ï¸ Setup Environnement

```bash
make install      # Installe les dÃ©pendances dans un venv
source .venv/bin/activate
```

## ğŸš€ Pour aller plus loin

-

---

âœ‰ï¸ Auteur : Jaouad Tanouti

Formation M2i Data Engineer - Projet E-commerce pipeline de donnÃ©es

